Introducing this project, I defined my goal as being to produce a process with which to generate a individualised HRTF set based on user-generated data, without any more equipment or expertise than the average end user of a virtual, augmented, or mixed reality system might have, on a practical time scale. The process as it currently stands does not meet these requirements. However I am confident that it is a good starting point. From a user experience standpoint, basing the individualisation process on the user's error in localisation is an interesting approach that has definite merit. There is sizable scope for gamification of the process using this method, which has the potential to encourage non-enthusiast users to undergo the process as these technologies become more widespread and their application areas become more diverse. To make it practical, however, the underlying algorithm would need to produce an individualised HRTF using a smaller amount of data than is currently required. Using the current implementation, even if the rate of measurements were to be increased tenfold it would still take an untenably long time to produce an entire HRTF set, failing to achieve one of the potential improvements of this type of method over the traditional measurement process. 

This leads into the greatest current problem with the produced implementation; it is unknown whether or not it would actually produce an effective HRTF set. This is part of the problem of choosing to base this project on SA, as discussed in previous chapters. Making pseudorandom changes to the HRTF data means that sub-optimal child states are regularly explored, and this slows the process down significantly. Given that my listening tests were limited to exploring six steps into the search process, it is difficult to estimate how many steps through the search would be required to reach a point at which the HRTF could be said to be fit the individual. Knowing, however, that it is greater than six means that it can be conluded that individualisation using this exact implementation is untenable.

\section{Improvements}

It is possible that some small modifications could make this method more effective, starting with the effect on perception that result from the adjustment value. The current implementation was based on Josef Holzl's implementation, that allowed adding or subtracting values up to three from the PCWs, but a more concerted look into the effects of different ways of calculating the adjustment value could be beneficial. It would also be worth reviewing the effect that different starting HRTF sets have on the process. As has been mentioned previously, it's impossible to find HRTF data that works for everyone. But it might be possible to find a small set that work broadly as starting points for individualisation. The starting HRTF could even be selected automatically based on a similar style of localisation test. 

It has already been covered heavily in previous chapters, but a better understanding of the relationship between the PCW adjustments being made and the effect that the adjustments will have on perception could be the greatest improvement while keeping the broader implementation details largely the same. Coupled with a better understanding of the change produced by a given adjustment value, this could help eliminate the early missteps in the search process and allow the algorithm to make more targeted adjustments sooner, lowering the total number of steps required for individualisation.

Working from the assumption that enough data can be gathered, potentially through use of or research into a greatly improved version of the current algorithm, I think there is also scope to investigate the application of machine learning to this process. There are plenty of obstacles ahead of making this feasible, but assuming they could all be overcome with sufficient research the ideal implementation might function as follows: As with the current implementation, it should start with a neutral/default HRTF, play the user a set of sounds from a predetermined set of positions, and ask them to locate where they believe they originated from. This data should be enough (the exact number quantified with the design of the system) to feed into a pre-trained machine learning model that should then return a more individualised HRTF. Training such a model would take a great deal of data, and generating that data could be problematic. But if the ability to capture that data was built into a simpler system that was actively used, it could eventually be feasible. Such a system would fulfil all of the requirements that I initially defined, while hopefully offering improved an improved spatial audio experience for a wider range of people.
