TODO:
sort out subsubsection topics to make it more elegant,
fill out notes'd bits,
should be ntroduction, can include sections on background, the problem being solved/my motivation, and my proposed solution method. 


\subsubsection{How Humans Localise Sound/What do I mean by spatial audio?}
Humans localise sound in a number of ways, 

add in a bit about this, citing blauert, ez

humans are binaural, not monaural 

When HRTFs are applied to an arbitrary signal
and presented to the listener’s two ears through headphones,
he or she hears a ‘‘virtual target’’ that appears to originate
from the location of the original sound source ?e.g., Wight-man and Kistler, 1989b; Bronkhorst, 1995; Møller et al.,
1996?.

users inability to localise using someone else's hrtfs in: \citep{Middlebrooks1999a}

\subsubsection{Present Context}
Spatial audio has never been more important. Though there has been a steady stream of interest in applications of spatial audio in fields like defence - primarily applied to Virtual Auditory Displays (VADs)\citep{•} - virtual, augmented, and mixed reality form a large component of the current technological zeitgeist. One major stated goal of these technologies is that of immersion. This doesn't have to mean that the user feels as if they have been transported to somewhere completely new, they just have to believe in the virtual elements of the experience. Anecdotes about users gingerly walking around virtual holes, or skirting a table that they know was not in the room before they put the headset on abound. No matter whether the technology is designed for entertainment, enterprise use, or to assist, the end user must be deceived into believing in what they are experiencing. 

For virtual reality to be convincing, audio must have parity with visuals - just small errors in either can irreparably break immersion \citep{•}. [FLESH THIS OUT]
listeners often complain that auditory events are spatially diffuse, and listeners often make incorrect judgements regarding the source locations \citep{•} (Wenzel et al., 1993; Møller et al., 1996?.)

In the real world, humans learn to localise sound sources based on a number of cues naturally encoded in the audio signals arriving at their inner ear. The simplest example of which, inter-aural level difference, or ILD, merely refers to the difference in volume between the listener's left and right ears. Cues like these rely on the fact that humans have two ears, that they are binaural, and so the most effective implementations of spatial audio for virtual reality and other similar technologies attempt to mimic these cues, by filtering audio signals that are mixed down into a two channel audio feed, intended for consumption through headphones. [CITATIONS?]

Reproducing spatial audio convincingly involves a number of factors, including reflections and occlusion caused by the room and the objects in it. This project however, will focus entirely on the effect the anthropometry of the listener has on the audio signal - the attenuation of the sound caused by the various body parts that they sound waves come into contact with, as well as the inter-aural differential cues such as ITD and ILD.

\subsubsection{Modeling Sound Localisation} %merge with the first section?
In most applications involving spatial audio, representing this attenuation is done using Head-Related Transfer Functions, or HRTFs, derived from their time-domain counterparts Head-Related Impulse Responses, or HRIRs. HRTFs are a model for representing this effect on a given signal that the listener's morphology has, and this model can, in theory, be used to convincingly render audio spatially \citep{•}. HRIR measurements are taken by placing microphones in the ears of a participant (human or mannequin) and measuring the impulse response resulting when a tone is played from a loudspeaker \citep{who did this first?}. This measurement process should be repeated for as many positions/points of origin around the participant as desired, but can involve as many as 1550 source positions in the case of the ARI\citep{ari database} and SADIE\citep{sadie db} databases.This process is incredibly labour-intensive, requires specialist equipment, and can take hours to perform. As a result, there are few organisations capable of performing these measurements, and generating a set of HRTFs for most individuals is impractical at best. There are a few organisations that have assembled databases of HRTFs or HRIRs, that involve measurements from a range of participants. Typically, the two main differences in these databases are the number of source positions, and the number of participants involved.

\paragraph{Source Positions:}The number of source positions varies from database to database [in the case of CIPIC it is every L degrees from N to M, in the case of ARI, it is every Y degree from X to Z, etc]
[add diagrams!]

\paragraph{Subjects:}
These databases may contain anything from data from a single mannequin in the case of the MIT KEMAR set \citep{Gardner1994}, to the CIPIC database's 45 subjects \citep{Algazi2001}, up to the 110-subjects-and-growing ARI HRTF database \citep{AcousticsResearchInstitute}. 

\paragraph{•}
[table of databases by subjects and participants maybe?]

\subsubsection{The Problem}
Because of the aforementioned difficulty in measuring HRTFs, data from these databases is commonly used in attempts to implement spatial audio solutions. Either a  In the simplest implementations, the audio sample is convolved with the HRIR, producing audio that appears to come, convincingly or otherwise, from the position in 3D space that the HRIR was originally measured from. 

The problem with using this data in any spatial audio implementations that are to be used in applications for the consumption of a wide range of end users, is that HRTF data is incredibly specific to the person the measurements have been taken from. Just small differences in the anthropometry of the measured participant and the end user can compromise the efficacy of the HRTF used\citep{•}. However, when one tries instead to use a generalised HRTF - derived from the average of a set of measurements, or from a mannequin like the KEMAR\citep{kemar mannequin?} - the processed audio is ineffective in much the same way that it is when using HRTFs measured from another person. The KEMAR, by dint of being a mannequin with average features, doesn't work for anyone who is not in possession of a totally average morophology. When using HRTFs that are not well matched to the user, front/back and elevation confusion is very common \citep{•}. 

Re-mention that VR/AR is popular, and that audio is integral to the progression of the technology!!!!.

It follows, then, that in a system that implements binaural audio, the audio for a user would be processed using a set of HRTFs could be used to recreate audio in such a manner that the user would be able to accurately localise the source of a sound. As we have already established, the traditional method of measuring HRTFs is impractical for the vast majority of users, which leaves us at something of an impasse. We need a method for producing individualised sets of HRTFs with minimal specialist equipment, an easy user experience that does not require expert knowledge, as small a time investment as possible. 

\subsubsection{Proposed Solution?}