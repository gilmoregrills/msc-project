TODO:
sort out subsubsection topics to make it more elegant,
fill out notes'd bits,
should be ntroduction, can include sections on background, the problem being solved/my motivation, and my proposed solution method. 

Notes: 

When HRTFs are applied to an arbitrary signal
and presented to the listener’s two ears through headphones,
he or she hears a ‘‘virtual target’’ that appears to originate
from the location of the original sound source ?e.g., Wight-man and Kistler, 1989b; Bronkhorst, 1995; Møller et al.,
1996?.



\subsubsection{Present Context}
Spatial audio has never been more important. Though there has been a steady stream of interest in applications of spatial audio in fields like defence - primarily applied to Virtual Auditory Displays (VADs)\citep{•} - virtual, augmented, and mixed reality form a large component of the current technological zeitgeist. One major stated goal of these technologies is that of immersion. This doesn't have to mean that the user feels as if they have been transported to somewhere completely new, they just have to believe in the virtual elements of the experience. Anecdotes about users gingerly walking around virtual holes, or skirting an object that they know was not in the room before they put the headset on, abound. No matter whether the technology is designed for entertainment, enterprise use, or to assist, the end user must be deceived into believing in what they are experiencing. 

For virtual reality to be convincing, audio must have parity with visuals - just small errors in either can irreparably break immersion \citep{•}. Listeners often complain that auditory events are spatially diffuse, and listeners often make incorrect judgements regarding the source locations \citep{•} (Wenzel et al., 1993; Møller et al., 1996?.)

In the real world, humans learn to localise sound sources based on a number of cues naturally encoded in the audio signals arriving at their inner ear. The simplest example of which, inter-aural level difference, or ILD, merely refers to the difference in volume between the listener's left and right ears. Cues like these rely on the fact that humans have two ears, that they are binaural, and so the most effective implementations of spatial audio for virtual reality and other similar technologies attempt to mimic these cues, by filtering audio signals that are mixed down into a two channel - binaural - audio feed, intended for consumption through headphones. \citep{}

Reproducing spatial audio convincingly involves a number of factors, including reflections and occlusion caused by the room and the objects in it. This project however, will focus entirely on the effect the anthropometry of the listener has on the audio signal - the attenuation of the audio signal caused by the various body parts that they sound waves come into contact with, as well as the inter-aural differential cues such as ITD and ILD.

\subsubsection{Modeling Sound Localisation} %merge with the first section?
In most applications involving spatial audio, representing this attenuation is done using Head-Related Transfer Functions, or HRTFs, derived from their time-domain counterparts Head-Related Impulse Responses, or HRIRs. HRTFs are a model for representing this effect on a given signal that the listener's morphology has, and this model can, in theory, be used to convincingly render audio spatially \citep{•}. HRIR measurements are taken by placing microphones in the ears of a participant (human or mannequin) and measuring the impulse response resulting when a tone is played from a loudspeaker \citep{who did this first?}. This measurement process should be repeated for as many positions/points of origin around the participant as desired for a given implementation, but can involve as many as 1550 source positions in the case of the ARI\citep{ari database} and SADIE\citep{sadie db} databases.This process is incredibly labour-intensive, requires specialist equipment, and can take hours to perform. As a result, there are few organisations capable of performing these measurements, and generating a set of HRTFs for most individuals is impractical at best. There are a few organisations that have assembled databases of HRTFs or HRIRs, that involve measurements from a range of participants. Typically, the two main differences in these databases are the number of source positions, and the number of participants involved.

\paragraph{Source Positions:}The number of source positions varies from database to database [in the case of CIPIC it is every L degrees from N to M, in the case of ARI, it is every Y degree from X to Z, etc]
[add diagrams!]

\paragraph{Subjects:}
These databases may contain anything from data from a single mannequin in the case of the MIT KEMAR set \citep{Gardner1994}, to the CIPIC database's 45 subjects \citep{Algazi2001}, up to the 110-subjects-and-growing ARI HRTF database \citep{AcousticsResearchInstitute}. 

\paragraph{•}
[table of databases by subjects and participants maybe?]

\subsubsection{The Problem}
Because of the aforementioned difficulty in measuring HRTFs, data from these databases is commonly used in attempts to implement spatial audio solutions. Either a participant from the database who is deemed to be sufficiently average in their morphology, a selection of participants, or an HRTF set derived from average values for the participants in the database may be used. In the simplest implementations, the audio sample is then convolved with the HRIR, producing audio that appears to come, convincingly or otherwise, from the position in 3D space that the HRIR was originally measured from. The difficult tasks is then to interpolate between these HRIRs in real time in response to the movements of the user (and potentially the source too).  

The problem with using this data in any spatial audio implementations that are to be used in applications for the consumption of a wide range of end users, is that HRTF data is incredibly specific to the person the measurements have been taken from. Just small differences in the anthropometry of the measured participant and the end user can compromise the efficacy of the HRTF used\citep{Middlebrooks1999a}. However, when one tries instead to use a generalised HRTF - derived from the average of a set of measurements, or from a mannequin like the KEMAR\citep{kemar mannequin?} - the processed audio is ineffective in much the same way that it is when using HRTFs measured from another person. The KEMAR, by dint of being a mannequin with average features, will not be effective for anyone who is not in possession of a totally average morophology. When using HRTFs that are not well matched to the user, front/back and elevation confusion in particular is very pronounced \citep{•}. 

It follows, then, that in a system that implements HRTF-based binaural audio, the audio for a user would be processed using a set of HRTFs that matched the user well enough that the resulting audio would enable the user to accurately localise the source of a sound. As we have already established, the traditional method of measuring HRTFs is impractical for the vast majority of users, which leaves us at something of an impasse. We need a method for producing individualised sets of HRTFs with minimal specialist equipment, an easy user experience that does not require expert knowledge, as small a time investment as possible. 

\subsubsection{Proposed Solution?}
The method that I am proposing would involve modifying an existing HRTF set so as to better fit a particular user, based on data that can be generated by the user, within a virtual or mixed reality environment. This method assumes the user has access to a virtual reality headset/head mounted display of some kind, and the intention is to have the user attempt to locate the source of audio cues that are played to them, within a virtual 3D environment. The data this generates, the difference between the perceived source of the sound and the actual sound source, is what adjustments to the HRTF should be made based upon. This process may then continue until the user starts to successfully localise the sounds sources, or until the error rate drops below a certain boundary. 

This frames the task of HRTF individualisation as an optimisation problem. The goal of a process like the one outlined above being to make certain values, representing the difference between perceived and actual sources, as small as possible. This is of course the implicit goal of every HRTF individualisation method, but including it as a variable in the process allows us to consider adapting existing optimisation search solutions[wording?] for use in this context. During the next chapter I will investigate some existing algorithms, and attempt to gauge their efficacy. The next chapter will also investigate some of the existing models for understanding HRTFs and methods for attaining individualised sets, and whether or not they are applicable to the proposed method. 




