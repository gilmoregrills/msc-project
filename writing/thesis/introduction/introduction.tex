Spatial audio has never been more important. Though there has been a steady stream of interest in applications of spatial audio in fields like defence - primarily applied to Virtual Auditory Displays (VADs)\citep{bronkhorst} - virtual, augmented, and mixed reality form a large component of the current technological zeitgeist. One major stated goal of these technologies is that of immersion. This doesn't have to mean that the user feels as if they have been transported to somewhere completely new, they just have to believe in the virtual elements of the experience. To illustrate this, anecdotes about users gingerly walking around virtual holes, or skirting an object that they know was not in the room before they put the headset on, abound. No matter whether the technology is designed for entertainment, enterprise use, or to assist, the end user must be deceived into believing in what they are experiencing. 

For virtual reality to be convincing, audio must at a minimum have parity with visuals as just small errors in either can irreparably damage immersion. Listeners regularly complain that auditory events are spatially diffuse, and often make incorrect judgements regarding the source locations \citep{wenzel1}.

In the real world, humans learn to localise sound sources based on a number of cues naturally encoded in the audio signals arriving at their inner ear. The simplest example of which, inter-aural level difference, or ILD, merely refers to the difference in volume between the listener's left and right ears. Cues like these rely on the fact that humans have two ears, that they are binaural, and so the most effective implementations of spatial audio for virtual reality and other similar technologies attempt to mimic these cues. This is achieved by filtering audio signals that are mixed down into a two channel, also binaural, audio feed, intended for consumption through headphones \citep{Algazi2011}.

Reproducing spatial audio convincingly involves a number of factors, including reflections and occlusion caused by the room and the objects in it. This project, however, will focus entirely on the effect the anthropometry of the listener has on the audio signal. That is, the attenuation of the audio signal caused by the various body parts that the sound waves come into contact with, as well as the inter-aural differential cues such as ITD and ILD.

\section{Modeling Sound Localisation} %merge with the first section?
In most applications involving spatial audio, representing this attenuation is done using Head-Related Transfer Functions (HRTFs) derived from their time-domain counterparts Head-Related Impulse Responses (HRIRs). HRTFs are a model for representing this effect on a given signal that the listener's morphology has, and this model can, in theory, be used to convincingly render audio spatially \citep{Bronkhorst1995} \citep{Begault2000}. HRIR measurements are taken by placing microphones in the ears of a participant (human or mannequin) and measuring the impulse response resulting when a tone is played from a loudspeaker \citep{gardner1995} \citep{Algazi2001}. This measurement process should be repeated for as many positions/points of origin around the participant as desired for a given implementation, but can involve as many as 1550 source positions in the case of the ARI\citep{arisite} and SADIE\citep{sadiesite} databases.This process is incredibly labour-intensive, requires specialist equipment, and can take hours to perform. As a result, there are few organisations capable of performing these measurements, and generating a set of HRTFs for most individuals is impractical at best. There are a few organisations that have assembled databases of HRTFs or HRIRs that involve measurements from a range of participants. Typically, the two main differences in these databases are the number of source positions, and the number of participants involved.

\section{The Problem}
Because of the aforementioned difficulty in measuring HRTFs, data from these databases is commonly used in attempts to implement spatial audio solutions. Either a participant from the database who is deemed to be sufficiently average in their morphology, a selection of participants, or an HRTF set derived from average values for the participants in the database may be used. In the simplest implementations the audio sample is then convolved with the HRIR, producing audio that appears to come, convincingly or otherwise, from the position in 3D space that the HRIR was originally measured from. The difficult task in this case is then to interpolate between these HRIRs in real time in response to the movements of the user (and potentially the source too).  

The problem with using this data in any spatial audio implementations that are to be used in applications for the consumption of a wide range of end users, is that HRTF data is incredibly specific to the person the measurements have been taken from. Just small differences in the anthropometry of the measured participant and the end user can compromise the efficacy of the HRTF used\citep{Middlebrooks1999}. However, when one tries instead to use a generalised HRTF - derived from the average of a set of measurements, or from a mannequin like the KEMAR - the processed audio is ineffective in much the same way that it is when using HRTFs measured from another person. The KEMAR, by dint of being a mannequin with average features, will not be effective for anyone who is not in possession of a totally average morphology. When using HRTFs that are not well matched to the user, front/back and elevation confusion in particular is very pronounced \citep{wenzel1}. 

It follows, then, that in a system that implements HRTF-based binaural audio, the audio for a user would be processed using a set of HRTFs that matched the user well enough that the resulting audio would enable the user to accurately localise the source of a sound. As we have already established, the traditional method of measuring HRTFs is impractical for the vast majority of users, which leaves us at something of an impasse. We need a method for producing individualised sets of HRTFs with minimal specialist equipment, an easy user experience that does not require expert knowledge, as small a time investment as possible. 

\section{Proposed Solution}
The method that I am proposing is to modify an existing HRTF set so as to better fit a particular user, based on data that can be generated by the user, within a virtual or mixed reality environment. This method assumes the user has access to a virtual reality headset/head mounted display of some kind, and the user will be asked to attempt to locate the invisible source of audio cues that are played to them, within a virtual 3D environment. The data this generates (the difference between the perceived source of the sound and the actual sound source) is what adjustments to the HRTF should be made based upon. This process may then continue until the user starts to successfully localise the sounds sources, or until the error rate drops below a certain boundary. 

This frames the task of HRTF individualisation as an optimisation problem. The goal of a process like the one outlined above being to make certain values, representing the difference between perceived and actual sources, as small as possible. This is the implicit goal of every HRTF individualisation method, but including it as a variable in the process allows us to consider adapting existing optimisation search solutions for use in this context. In chapter two I will investigate some existing algorithms, and attempt to gauge their efficacy. This chapter also investigates some of the existing models for understanding HRTFs and methods for attaining individualised sets, and whether or not they are applicable to the proposed method. Chapter three details how the proposed solution has been implemented and tested, as well as notable considerations and decisions that had to be made, and the technologies involved. Chapter four covers the analysis of the data from the listening tests, both in order to ascertain whether or not the project can be deemed a success and to identify data that may be used to inform future updates to this work. Lastly, chapter five dicusses these findings in comparison to the stated goals of this research, as well as proposing some key improvements that may be applied to future implementations of similar systems. 




