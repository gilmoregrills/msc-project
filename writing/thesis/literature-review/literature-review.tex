This idea of generating individualised sets of HRTFs without having to perform the complex measurements that would usually be required has existed since the 1990s \citep{Kistler1992}. The ideal scenario for commonplace spatial audio involves every user having access to an HRTF set that works for them. If traditional methods of measurements are impractical, then alternatives are necessary.

\section{Methods}
Investigations into HRTF individualisation have been done using a range of methodologies, some involving just simple selection tasks \citep{Zotkin2002} and others complex tuning itep{Tan1998} - adjusting multiple parameters against listening tests. Often these methods hinge on a specific model that is used to decompose the HRTF into individual parameters. These can be manipulated independently in order to achieve meaningful control over the customisation process. In some cases these models also seek to make clear the relationship between the features of the HRTF and the features of the user - the morphological properties of the participant being the primary determinant of a given HRTF this seems a logical approach. In these next few sections I will cover the main of the approaches that have been investigated to date, as well as their efficacy and why they are or are not well suited to this project. I will demonstrate that there is a definite overlap between these methods, leading to the idea that perhaps in a more comprehensive but laborious model for HRTF individualisation, a combination of these techniques might be used \citep{hoene2017}.

\subsection{Database Matching}
Database matching is often incorporated into other models for HRTF individualisation. It is based on the predicate that within a database of a given size, there must be a set of HRTF measurements that have been taken from a participant with similar anthropometric features as a given user. This technique has been used in a range of studies on spatial audio, both as part of a wider study on binaural audio and localisation, \citep{Zotkin2002} and as the sole focus of the study \citep{Zotkin}. As in both of these papers from Zotkin et al, many attempts to match participants with closely matching HRTF sets use measurements of the user's anthropometry, which they will then try to match to the anthropometric measurements taken in the process of assembling the database. 

This can work reasonably well, assuming the database used contains measurements from a great enough range of people. The CIPIC database contains anthropometric measurements for all 45 of its participants \citep{Algazi2001}, while the ARI database comes with measurements for 50 of its participants \citep{AcousticsResearchInstitute}. Problems  with this method can of course arise when the database does not include measurements from a participant with a morphology that does not closely match those of the user. The second problem with this method is more of an issue when considering this method in terms of what this project hopes to achieve. Given my stated requirements, any method that requires precise anthropometric measurements is problematic. This is due to both the difficulty of performing the measurements effectively, and the effort that such an act involves, the use of either of which would not satisfy any of my self-imposed user experience standards.

An alternative method for matching users to their closest-matching HRTF set could be based on subjective listening tests. Playing a user a sample, filtered using an HRTF taken from a database, and asking them to indicate where they believed the sound came from. This process can be repeated for as many examples as are contained in the database, and the one that results in the least incorrect localisation attempts chosen. The problems with this method are again clear, in that the labour required to search all the entries in a database is more than anyone but the most die-hard users are likely to pursue. Improvements are made on these kinds of subjective listening tests, however, in attempts to match users to more appropriate HRTFs through the clustering of similar sets. 

\subsection{Clustering}
Clustering involves collating a database of HRTF sets measured from different participants, and then sorting these into orthogonal groups based on a specific feature. Fahn and Lo \citep{Fahn2003} grouped HRTFs based on the power cepstra of each HRTF set. They then used a modified version of the LBG algorithm to form 6 different clusters. Other studies, such as Xie et al \citep{xie2013a} found a total of 7 clusters were required. Either way, the idea is to group HRTFs into groups - or clusters - where each HRTF is similar enough to the others in the cluster, but where the differences between each cluster are sufficiently great. The central example can then be taken from each cluster, the HRTF that best represents that cluster or that represents the average, and provide to the end user the example from this set of 6 or 7 that best matches them. This approach appears largely effective, Shimada et al \citep{shimada1994a} found that their participants exhibited significantly improved localisation performance  using their clustering method. 

Given that clustering is meant to facilitate the process of matching a user with a more personal HRTF set, trying to match users by anthropometry again would be nonsensical. Instead, subjective listening tests are used more often \citep{xie2013a}. Using this method, the comparative efficacy of subejctive tests in this instance is clear versus raw database matching. As opposed to subjecting an unending barrage of tests against 45 or more (as a slightly facetious example), the listener has to only compare between 6 or 7. However, the resulting localisation is going to be less precise, given the inherently more generalised approach. The increase in user-friendliness it interesting, though. In lighter-weight applications of VR/AR, perhaps for example on mobile devices, this approach could work. Giving interested users the option to choose between a subset of sufficiently disparate HRTFs, adding a little lightweight customisation.

\subsection{Frequency Scaling}
Another methodology is based upon scaling in frequency entire HRTFs or elements of the HRTF. A method that was investigated early on in attempts to devise individualisation methods, it is one that lost out to cluster/database matching methods in the longer run.

Some notable examples of studies into this technique include one by Middlebrooks \citep{Middlebrooks1999a}. In this study, they used Directional Transfer Functions (DTFs) which are processed HRTFs with the source location information isolated \citep{middlebrooks1990}. Initially finding that spectral features from one participant's DTF could be aligned with those of another by scaling. In further investigation participants used DTFs from the other participants, which were then scaled by a range of different factors based on comparisons in the two participants anthropometry - primarily the size of the head, and pinnae. This study then compared the participant's ability to localise sounds convolved with another's DTF against localisation when using the scaled DTFs and found a roughly 50\% increase in accuracy with the most effective scale factor. 

Another method investigated by Tan et al \citep{Tan1998}, involved building a tool that allowed users to manipulate the scaling of an HRTF themselves. Given that front/back and elevation confusion is most common when using non-individual HRTFs, they opted to provide options to add a bias towards the front/back, as well as another parameter to tweak how elevation was perceived. Their results showed a small improvement over the non-individualised sets, but the results varied between participants; given the simplicity of adjusting a mere two parameters this approach could have been very convenient. But the lack of an impressive improvement in localisation makes it a less tenable solution than some of the others explored, and it is overshadowed by later methods. 

\subsection{Structural Models}
Structural models appear to be the most commonly studied models for understanding HRTFs as well as for attempting to synthesise individual sets or customise generalised sets\citep{Brown1998}.  Because HRTFs and HRIRs represent the affects on the sound signal/wave of the features of a human's body, then one should be able to extract and isolate the discrete elements an HRTF that relate to the individual body parts. Klaus Genuit first proposed a model for understanding HRTFs as a series of filters that each represented the effects of a certain anatomical feature \citep{Genuit1984}. The idea of a structural model, or of HRTF individualisation based on a user's anthropometry is pervasive, and many other methods incorporate elements from it. For example, the aforementioned 2003 study by Zotkin, Duraiswami, Davis, and Hwang  \citep{Duraiswami2003} used anthropometric measurements to match a user to closely-matching set of HRTFs from the CIPIC database. Similarly, later studies centered on Principal Components Analysis - discussed further in the next section - look at the relationship between principal components (PCs) and morphological features. 

Work by Brown and Duda \citep{PhillipBrown1998} (itself based on a 1996 paper \citep{lopexmeddis1996}) looked primarily at HRIRs, focusing on the additional temporal information that the frequency-domain HRTFs lacked. The decision to focus on the time domain was to allow them to identify the characteristics of HRTFs that are the result of the different paths to the inner ear that the sound waves took, over time. This study involved only a small number of participants, and so whether or not the synthesised HRTFs produced with this model could replace measured ones was left to more comprehensive studies. 

In a 2001 study Algazi, Duda, Morrison, and Thompson attempted to produce an approximated HRTF from the isolated responses of different structural components \citep{Algazi2001a}. As with other studies, the synthesis was performed based on anthropometric measurements of the subjects, and the final HRTF composite - made up of the responses of each structural component. This approach was evaluated using a composite HRTF vs a measured HRTF, and when viewed spectrally the two had significant similarities. The study did not go as far as to perform subjective/psychoacoustic tests, however. A similar study in 2003 by Raykar and Duraiswami \citep{Raykar2003} aimed to decompose the HRTF into a set of significant features that are integral to the localisation of sound sources. Their results were promising, developing an algorithm to decompose a given HRTF, and testing it successfully on every participant in the CIPIC database. 

This model often provides promising results, and can be well-suited for applications where a high level of localisation accuracy is required, but a full measurement session is out of the question for the proposed implementation. The main problem with this method is the precision that is required for the measurements. If an ideal implementation for widespread consumer use relies on a simple calibration process, detailed measurements it becomes more difficult to fulfil that requirement. Investigations have been made into the use of computer vision to automate the measurement process and eliminate human error\citep{Mohan2003}, but unless this process can be distilled into something simple that requires minimal additional hardware, for example using just a smartphone camera, then it is sub-optimal for widespread use. 

More recent studies have tried to combine this approach with others, using a combination of PCA and a reduced number of anthropometric measurements \citep{Hugeng2010}. Or a similarly reduced number of measurements to match a set of HRTFs to a subject, then modify them to improve their performance \citep{Tashev2014}. Even trying a combination of structural data and a radial basis function (RBF) nerual network \citep{Li2013}.  

\subsection{Principal Components Analysis}
Principal Components Analysis \citep{Pearson1901} (PCA) is a process for analysing a dataset and and identifying the principal components of that dataset. It is a statistical procedure that attempts to return an efficient representation of the dataset, turning the dataset from some large number of variables, into a smaller collection of principal components (PCs), adding some more efficient structure to the dataset. 

\subsubsection{Understanding PCA}
PCA can be used to decrease the number of dimensions in a dataset; reducing it down to its most basic components. The dimensionality of a dataset is identified by the number of variables present in that dataset. In general terms, for our current example (that of an HRTF set) the data should represented as a matrix - the structure of the matrix is dependent on the approach being taken, and different approaches will be explored later in this section. As an example, Kistler and Wightman \citep{Kistler1992} used a data set arranged into a 5300x150 matrix. If one were performing PCA manually, this dataset could be then be decomposed into a collection of pairs of eigenvectors and eigenvalues - each pair represented by a line through the dataset at the point of greatest variance. Each pair is comprised of a direction and the variance of the data along that line - the vector and the value, respectively. The number of eigenvector/value pairs in a dataset directly corresponds to the dimensionality of the dataset. The dimensions (eigenvectors) with the greatest variance (highest eigenvalues) will consistute the principle components. The number of principle components chosen from the resulting set is dependent on the level of detail necessary. In the aforementioned Kistler and Wightman study, it was found that 90\% of the HRTF could be reconstructed using only the five PCs with the highest level of variance. The Principal Component Weights, or PCWs, are the individual values around an eigenvector - the variance between which gives us the eigenvalue.


\subsubsection{Individualising HRTFs/HRIR with PCA}
There are a few main differences among studies that apply principal component analysis to HRTFs and HRIRs. Chief among them is whether the study uses HRTFs \citep{Holzl2014a} \citep{Gutierrez-Parera2017} or HRIRs \citep{Hwang2008} \citep{Hwang2007} \citep{Fink2012}. There are benefits of working solely with HRIRs, one retains the interaural time difference (ITD), and it's easier to extract the effects of subject anthropometry on the resultant HRIR. However when analysing HRIRs with PCA, researchers often time-align the HRIRs before processing \citep{hwang2010customization}, losing information on ITD. Estimation of ITD is a comparatively trivial task, and when not relying on an anthropometric model for individualisation the correlation between HRIR and anthropometric features becomes irrelevant. One advantage of the use of HRTFs that cannot be overstated is the larger corpus of research already done on the work \citep{Holzl2014a}. 

In an early paper on the subject, Middlebrooks \citep{middlebrooks1992observations} found that no matter the database used, the amount of variance described by each PC, and the number of PCs required for a substantial reconstruction of the original HRTF was more or less equal. This is helpful, allowing the results of any investigation into a particular method to be applied semi-interchangably to new research. The number of PCs used for reconstruction varies a lot, however - anywhere from 4 \citep{martens1987principal} to 90 \citep{martens1987principal}. The decision as to how many PCs to use depends heavily on the intended accuracy of the reconstruction. The more PCs that are used, the more accurate any reconstruction based on them would be. There are, however, heavily diminishing returns on this number. Many studies are in agreement that using 4-6 PCs can describe around 90\% of the variance within the HRTF set \citep{martens1987principal} \citep{Kistler1992}. It is common to want to reduce the number of PCs used, so as to in turn reduce the complexity of the tuning process \citep{Hwang2007}. When this tuning process is manual, this concern/focus is understandable. If the process is automated, there could perhaps be a greater focus on accurate reconstruction.

Holzl \citep{Holzl2012} investigated the effect that different input matrices, created by restructuring the HRTF input data, had when performing PCA on HRTFs. In doing this he identified five different matrix arrangements used by different studies, and found the most effective to be [(\textit{subjects * sound directions}) x \textit{signal}] - a structure also used by Kistler and Wightman in their studies \citep{Kistler1992}. 

There is also a difference in how much of the dataset is analysed and adjusted at a time. Some studies \citep{Hwang2007} elect to just adjust a clustered sub-set of positions, playing a subject a sample from that direction and adjusting just those positions based on that. This process of performing PCA and updating the weights for each position can be time consuming. Other studies \citep{Holzl2014a} use a more global model, analysing all directions at once. For this method to be practical when performing manual adjustments, a method for modeling PCWs can be helpful. Holzl \citep{Holzl2014a} for example proposed a method for modelling PCWs based on the spherical harmonics transform. The proposed model effectively maps the PCWs to a sphere around the subject, allowing manipulation of the PCWs that apply to the intended apparent source of the sound being played to the user at a given time. There is a possibility that this approach could allow automated individualisation to be performed more effectively, adjusting the relevant datapoints in a more targeted way.

\section{Search Methods}

My proposed method for producing HRTFs is based upon being able to automate the adjustment of the principal component weights produced by using PCA on a generic set. The problem with developing a method for individualisation that works in this way is that there is little in the way of research looking at correlations between localisation errors and principal component weights - which is not altogether surprising. Because of this, I have instead chosen to investigate existing search algorithms that I might be able to fit to this problem. There is a possibility that the data generated by this project could be used to investigate such a correlation, and as such later updates to the process may be able to increase its efficacy. Because of the iterative and interactive nature of this method, any search will be inevitably slow. This is in part because there is no way yet of modelling or forecasting a user's response to a given HRTF-filtered audio sample. Any search method I use must be simple enough to implement easily, but also have the potential to produce a measurable change in localisation performance. Given the potential for incrementally adjusting every HRTF source position to result in a slow individualisation process, an algorithm that helps to alleviate that in any way will be preferable. I have no doubt that with more data regarding error rates and HRTF principal component weights a more sophisticated algorithm could be applied to this problem in the future, but for the purposes of implementing a functioning proof-of-concept a simple approach should be taken.

\subsection{Hill-Climbing Search}
The hill-climbing family of search methods \citep{norvig1995} start from a given potential solution to a problem, and attempt to find an optimal solution. For our case the steps are so: 
\begin{itemize}
\item Take a starting state - our generalised HRTF set.
\item This state is evaluated - the listener is played a sample and we test how well they can locate the source. 
\item A change is then made - a predetermined value is added to or subtracted from the parts of the HRTF that corresponds to the direction of the sound source being tested.
\item This state is then evaluated again, and the process repeats. 
\end{itemize} 

Hill-climbing is notorious for getting stuck in local maxima \citep{â€¢}, for example if we were to reach a point at which the user was failing to locate a sound source on their localisation attempts, but changes in either direction resulted in even greater error, a hill climbing algorithm would likely move between those points forever. There are ways to work around this limitation, including stochastic hill climbing \citep{stochastic} and random restart \citep{random restart}, a derivative of that. 

For my proposed implementation, hill climbing searches fit the simplicity requirements. Their suitability is more questionable when it comes to the rate of change. Child states in hill climbing search methods are usally generated by making a set change to the current state, deciding how to make these adjustments so that they were both small enough to generate a suitably precise degree of individualisation without it taking an untenably long time to get there. 

\subsection{Genetic Algorithms}
%Genetic Algorithms are a type of optimization search algorithm that incorporate operations inspired by evolutionary processes. Typically there are three main operations: Selection, mutation, and crossover. 
Genetic algorithms (GAs) \citep{Whitley1994} is a broad categorisation of a family of algorithms, whose operations mimic evolutionary processes. They have a degree more complexity than the other two algorithms mentioned here, but were quickly ommitted from consideration as something I could base the individualisation process on purely because it would not be practical to modify the steps involved for this use case. The reason they are worth mentioning at all is because the final implementation borrows one of the three primary operations involved in a typical GA. Broadly speaking, GAs work on a pool of candidate states. These states are generated in a pseudorandom way at the beginnig of the process, and evaluated to determine their fitness. A set pool of the fittest candidates are then selected as the next generation, usually referred to as the selection stage. This generation is then subjected to the following operations: 
\begin{itemize}
\item During the crossover stage, candidates are selected pseudrandomly from the current pool. Then at a random point within each candidate, the candidates are split, and half of one will be switched with half of another to create two new states. 
\item Then, during the mutation stage, elements of a subset of states may be changed. For example, if a given pool of states are represented as binary strings, the mutation may involve inverting a single bit somewhere in the string. Often the algorithm is steered toward mutating parts of the chosen states that would otherwise not be modified with crossover alone. 
\end{itemize}
Given that the evaluation function in this instance would have to be a user in a listening test, this would not be practical for this problem. I have however borrowed the basic functionality behind the mutation stage to inject some randomness into the makeup matrix that dictates whether the adjustment value is subtracted or added to a given PCW.


\subsection{Simulated Annealing}
Simulated Annealing \citep{vanLaarhoven1987} search is a simple iterative search method that requires a heuristic that measures how close a given state is to the goal state. The steps the algorithm takes are essentially as follows:
\begin{itemize}
\item From a starting state - a generalised HRTF.
\item The state is then evaluated by some evaluation function - in this case how close the user got to localising the source a given sample.
\item A new pseudorandom state is generated - in this case each PCW would be modified by a random amount, with the degree of randomness depending on the following:
\begin{itemize}
\item If the state was close to the goal state (if the user almost correctly identified the source of the samples) then use less randomness when generating successive states - use a smaller boundary when generating random amounts to adjust by.
\item Otherwise, generate a new state and start from there - allow greater variance in the values that are used to adjust the weights. 
\end{itemize}
\item This successive state - a modified HRTF set - should again be tested according to the evaluation function, as the process loops. 
\end{itemize}

This method is of course sub-optimal, because of its loose correlation between the error rates and the adjustments made to the PCs/PCWs. The fact that adjustments made to the HRTF are greater when the localisation error is higher, however, does have the potential to expedite the process. An implementation using this algorithm should produce an improved HRTF set for the user comparatively quickly, after a fewer number of iterations than something like hill climbing search. As such, it has the potential to make the proposed process more user-friendly, in that it is less laborious, while still theoretically having the ability to make small incremental improvements to a near-individualised HRTF set. For both these reasons and the fact that simulated annealing is a relatively simple algorithm to implement, my proof-of-concept implementation will be based primarily on SA.  

Some adjustments will need to be made, however, in order to increase the efficiency of the algorithm. If information about changes made to parent states and the error rates they produce is saved, it may help to avoid the problem of pursuing modifications that don't actually get closer to the goal state. This record of how much PCWs are adjusted and the error rates that are produced by those adjustments may help to add a bias to subsequent adjustments made to PCWs. This could limit the amount of randomness required in the adjustments, and may help us to reach the goal state of an individualised HRTF set faster. 

\subsection*{}
Based on the literature reviewed, an individualisation process centered around principal components analysis and simulated annealing search was selected. With the aforementioned user experience requirements, a method that allowed elements of the HRTF to be modified without information on the relationship between localisation errors and principal component weights was important. PCA and search methods like SA complement each other well too. The application of PCA serves to reduce the number values being manipulated in the serach process, and therefore the number of potential child states with each step in the search. Simulated annealing is being chosen in an effort to expedite the early stages of the search process while allowing for more micro changes as error rates lower. In the next chapter I will outline the final implementation being used to test the algorithm, as well as the problem-specific decisions that have been made in the process of building it. 