% File: specification.tex
% Created: Tuesday 24th January
% 
% MSc Project Specification

\documentclass[10pt, oneside, a4paper, draft]{scrartcl}

\begin{document}
\bibliography{/home/gilmoregrills/Documents/library.bib}
\title{HRTF Individualisation Based on Localization Errors in 3D Space}
\subtitle{Specification}
\author{Robin Yonge}
\date{Febuary 2017}
\maketitle

\renewcommand{\abtractname}{Introduction} %maybs change to blank
\begin {abstract}
Virtual reality\ignore{Computer-mediated reality \cite{S.Mann199}} has come to represent the dominant prediction for the near future of human-computer interaction. If this prediction is to come to pass, then it is necessary that sufficient consideration is given toward the fidelity of both the visual and the auditory aspects of VR and AR technology. Currently, most implementations of spatial audio in VR and AR applications are based around the use of HRTFs. Mostof these implementations use one of a few publically available datasets, derived from measurements performed on either human subjects or a model such a the KEMAR\cite{Algazi2001}\cite{Gardner1994}. For the use of VR/AR to become commonplace the experience must be universal. Because spatial audio relies on complex measurements of real-world subjects (be they flesh and blood or not) applications tend to use average or neutral HRTFs, ones that should work for the largest number of people. Of course, by dint of being average there are many people for whom these do not work. In order to achieve a universal experience, then, it is necessary to find a process for  simple individualization of spatial audio that is sufficiently simple, and can be performed by the end user as a part of the standardsetup of any VR/AR device or application.
\end {abstract}

\section*{}
The goal of this project is to produce a working prototype of an application that takes as input a neutral set of HRTF measurements, and returns an individualised set. For a process like this to achieve widespread use it must be simple enough that it can be performed by any given user with nothing more than a head-mounted display, and intuitive enough that it requires no in-depth knowledge of spatial audio. The output HRTF set should also provide a noticeable improvement in the ability of the user to locate sound sources in a virtual 3D environment.

%hrtf individualisation and synthesis methods through the ages? structural model, other weirder ones? Why are they bad?? Most recently PCA/PCW, this is what I will use, why? why adjust not synthesise? 

%on what basis will we make our adjustments? It will be done on the basis of localisation errors from the user. Depending on the nature of the error, we will adjust PCWs and re-test. 

%how do we record the nature of their localisation errors? We do it in-VR of course. Give the user a reticle and ask them to point it at the position in space that they think a given sound source eminated from. Use that data to adjust HRTFs and feed it back in! Play another sound, note error, adjust again, rinse and repeat. Informed guessing possibly on the basis of the structural model? Not totally blind. Float the idea of incorporating ML in future build. 

%How do we make this fun/enjoyable? Build it in an interesting 3D space, room of shelves all covered in alarm clocks? locate the clock that's beeping? 

%End product will be: A summary. 3 pieces, the hrtf databases, the processing module, and the VR frontend! 

\section*{}
%what technologies will I use and why?

\section*{}
this is section3

\end{document}


