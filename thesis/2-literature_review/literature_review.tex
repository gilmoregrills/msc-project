\subsection{Modifying HRTFs/Overview}
This idea of generating individualised sets of HRTFs without having to perform the complex measurements that would usually be required has existed since the 1990s \citep{Kulkarni1995}. The ideal scenario for commonplace spatial audio involves every user having access to an HRTF set that works for them. If traditional methods of measurements are impractical, then alternatives are necessary.

\subsection{Methods}
Investigations into HRTF individualisation have been done using a range methodologies, some involving just simple selection tasks and others complex tuning - adjusting multiple parameters against listening tests. Often these methods hinge on a specific model that is used to decompose the HRTF into individual parameters that can be manipulated independently in order to achieve meaningful control over the customisation process. In some cases these models also seek to make clear the relationship between the features of the HRTF and the features of the user - the morphological properties of the measuree being the primary determinant of generated HRIR this seems a logical approach. In these next few sections I will cover the main of the approaches that have been investigated to date, as well as their efficacy and why they are or are not well suited to this project. We will see that there is a definite overlap between these methods, leading to the idea that perhaps in a more comprehensive but laborious model for HRTF individualisation, a combination of these techniques might be used \citep{Hoene2017}.

\subsubsection{Database Matching}
Database matching is often incorporated into other models for HRTF individualisation, and is somewhat self-explanatory. It is based on the predicate that within a database of a given size, there must be a set of HRTF measurements that have been taken from a participant with similar anthropometric features as a given user. This technique has been used in a range of studies on spatial audio, both as part of a wider study on binaural audio and localisation, \citep{Zotkin2002} and as the sole focus of the study \citep{Zotkin}. As in both of these papers from Zotkin et al, many attempts to match participants with closely matching HRTF sets use measurements of the user's anthropometry, which they will then try to match to the anthropometric measurements taken in the process of assembling the database. 

This can work reasonably well, assuming the database used contains measurements from a great enough range of people. The CIPIC database contains anthropometric measurements for all 45 of its participants \citep{•}, while the ARI database comes with measurements for 50 of its participants \citep{ARI site docs}. Problems  with this method can of course arise when the database does not include measurements from a participant with a morphology that does not closely match those of the user. The second problem with this method is more of an issue when considering this method in terms of what this project hopes to achieve. Given the my requirements for the customisation method I intend to design, a method that requires precise measurements that would be difficult to perform at home is not ideal. A method that requires precise measurements to be taken has two problems, both in the difficulty of performing the measurements, and the effort that such an act involves, does not satisfy any of my self-imposed standards for user experience.

An alternative method for matching users to their closest-matching HRTF set could be based on subjective listening tests. Playing a user a sample, filtered using an HRTF taken from a database, and asking them to indicate where they believed the sound came from. This process can be repeated for as many examples as are contained in the database, and the one that results in the least incorrect localisation attempts chosen. The problems with this method are again clear, in that the labour required to search all the entries in a database is more than anyone but the most die-hard users are likely to pursue [citation for some human-computer interaction junk about how much effort people will put in?]. Improvements are made on these kinds of subjective listening tests, however, in attempts to match users to more appropriate HRTFs through the clustering of similar sets. 

\subsubsection{Clustering}
Clustering involves collating a database of HRTF sets measured from different participants, and then sorting these into orthogonal groups based on a specific feature. Fahn and Lo in their 2003 paper \citep{Fahn2003} grouped HRTFs based on the power cepstra of each HRTF set. They then used a modified version of the LBG algorithm to form 6 different clusters. Other studies, such as a 2013 paper by Xie et al \citep{xie2013a} found a total of 7 clusters were required. Either way, the idea is to group HRTFs into groups - or clusters - where each HRTF is similar enough to the others in the cluster, but where the differences between each cluster are sufficiently great. You can then take the central example from each cluster, the HRTF that best represents that cluster or that represents the average, and provide to the end user the example from this set of 6 or 7 that best matches them. 

Given that clustering is meant to make simpler the process of matching a user with a more personal HRTF set, trying to match users by anthropometry again would be nonsensical. Instead, subjective listening tests are used more often \citep{xie2013a} \citep{•}. Using this method, the comparative efficacy of subejctive tests in this instance is clear versus raw database matching. As opposed to subjecting an undending barrage of tests against 45 or more (as a slightly facetious example), the listener has to compare between 6 or 7. However, the resulting localisation is going to be less precise, given the inherently more generalised approach. The increase in user-friendliness it interesting, though. In lighter-weight applications of VR/AR, perhaps for example on mobile devices, this approach could work. Giving interested users the option to choose between a subset of sufficiently disparate HRTFs, adding a little lightweight customisation. 

\citep{doi:10.1080/00140131003675117}
\citep{shimada1994a}

\subsubsection{Frequency Scaling}
Another methodology is based upon scaling in frequency entire HRTFs or elements of the HRTF. A method that was investigated early on in attempts to devise individualisation methods, it is one that lost out to cluster/database matching methods in the longer run. 

Some notable examples of studies into this technique include one by Middlebrooks \citep{Middlebrooks1999a}. In this study, they used Directional Transfer Functions (DTFs) which are processed HRTFs with the source location information isolated \citep{middlebrooks1990}. Initially finding that spectral features from one participant's DTF could be aligned with those of another by scaling. In further investigation participants used DTFs from the other participants, which were then scaled by a range of different factors based on comparisons in the two participants anthropometry - primarily the size of the head, and pinnae. This study then compared the participant's ability to localise sounds convolved with another's DTF against localisation when using the scaled DTFs and found a roughly 50\% increase in accuracy with the most effective scale factor. 

Another method investigated by Tan et al \citep{Tan1998}, involved building a tool that allowed users to manipulate the scaling of an HRTF themselves. Given that front/back and elevation confusion is most common when using non-individual HRTFs, they opted to provide options to add a bias towards the front/back, as well as another parameter to tweak how elevation was perceived. Their results showed an improvement over the non-individualised HRTFs, but not a substantial enough one. Given the simplicity of adjusting a mere two parameters this approach could have been very convenient. But the lack of an impressive improvement in localisation makes it a less tenable solution than some of the others explored, and it is overshadowed by later methods. 

\subsubsection{Structural Models}
Structural models appear to be the most commonly studied models for understanding HRTFs as well as for attempting to synthesise individual sets or customise generalised sets[I have roughly a billion citations for this, should I just plonk them all here?].  Because HRTFs and HRIRs represent the affects on the sound signal/wave of the features of a human's body, then one should be able to extract and isolate the discrete elements an HRTF that relate to the individual body parts. A German researcher by the name of Klaus Genuit first proposed a model for understanding HRTFs as a series of filters that each represented the effects of a certain anatomical feature \citep{Genuit1984}. The idea of a structural model, or of HRTF individualisation based on a user's anthropometry is pervasive, and many other methods incorporate elements from it. For example the aforementioned 2003 study by Zotkin, Duraiswami, Davis, and Hwang  \citep{Duraiswami2003} used anthropometric measurements to match a user to closely-matching set of HRTFs from the CIPIC database. Similarly, later studies centered on Principal Components Analysis - something I will go into in more detail later - look at the relationship between principal components (PCs) and morphological features. 

1998 work by Philip Brown and Richard Duda \citep{PhillipBrown1998} (itself based on a 1996 paper \citep{lopexmeddis1996}) looked primarily at HRIRs, focusing on the additional temporal information that the frequency-domain HRTFs lacked. The decision to focus on the time domain was to allow them to identify the characteristics of HRTFs that are the result of the different paths to the inner ear that the sound waves took, over time. This study involved only a small number of participants, and so whether or not the synthesised HRTFs produced with this model could replace measured ones is left to more comprehensive studies. 

In a 2001 study Algazi, Duda, Morrison, and Thompson attempted to produce an approximated HRTF from the isolated responses of different structural components \citep{Algazi2001a}. As with other studies, the synthesis was performed based on anthropometric measurements of the subjects, and the final HRTF composite - made up of the responses of each structural component. This approach was evaluated using a composite HRTF vs a measured HRTF, and when viewed spectrally the two had significant similarities. The study did not go as far as to perform subjective/psychoacoustic tests, however. A similar study in 2003 by Raykar and Duraiswami \citep{Raykar2003} aimed to decompose the HRTF into a set of significant features that are integral to the localisation of sound sources. Their results were promising, developing an algorithm to decompose a given HRTF, and testing it successfully on every participant in the CIPIC database. 

This model often provides promising results, and can be well-suited for applications where a high level of localisation accuracy is required, but a full measurement session is out of the question. The main problem with this method being the precision that is required for the measurements. If an ideal implementation for widespread consumer use relies on a simple calibration process, detailed measurements it becomes more difficult to fulfil that requirement. Investigations have been made into the use of computer vision to automate the measurement process and eliminate human error\citep{•}[find citation again], but unless this process can be distilled into something simple that requires minimal additional hardware (perhaps a smartphone camera), then it is sub-optimal for widespread use. 

More recent studies have tried to combine this approach with others, using a combination of PCA and a reduced number of anthropometric measurements \citep{Hugeng2010}. Or a similarly reduced number of measurements to match a set of HRTFs to a subject, then modify them to improve their performance \citep{Tashev2014}. Even trying combination of structural data and a radial basis function (RBF) nerual network \citep{Li2013}.  

\subsubsection{Principal Components Analysis}
Principal Components Analysis \citep{Pearson1901} (PCA) is, as it sounds, a process for analysing a dataset and and identifying the principal components of that dataset. It is a statistical procedure that attempts to return an efficient representation of the dataset, turning the dataset from some large number of variables, into a smaller collection of principal components (PCs), adding some more efficient structure to the dataset. 

\paragraph{Understanding PCA}
[this section might be simplified too far/unnecessary but it's something I had to research so?]
So PCA can be used to reduce the number of dimensions in a dataset, reducing it down to its most basic components. The dimensionality of a dataset is identified by the number of variables present in that dataset. In general terms, for our current example (that of an HRTF set) the data should represented as a matrix - the structure of the matrix is dependent on the approach being taken, and different approaches will be explored later in this section. As an example, in the 1992 paper by Kistler and Wightman \citep{Kistler1992} the data set used was arranged into a 5300x150 matrix. If one were performing PCA manually, this dataset could be then be decomposed into a collection of pairs of eigenvectors and eigenvalues - each pair represented by a line through the dataset at the point of greatest variance. Each pair is comprised of a direction and the variance of the data along that line - the vector and the value, respectively. The number of eigenvector/value pairs in a dataset directly corresponds to the dimensionality of the dataset. The dimensions (eigenvectors) with the greatest variance (highest eigenvalues) will consistute the principle components. The number of principle components chosen from the resulting set is dependent on the level of detail necessary. In the aforementioned Kistler and Wightman study, it was found that 90\% of the HRTF could be reconstructed using only the five PCs with the highest level of variance. 


\paragraph{Individualising HRTFs/HRIR with PCA}
There are a few main differences among studies that apply principal component analysis to HRTFs and HRIRs. Chief among them is whether the study uses HRTFs \citep{Holzl2014a} \citep{Gutierrez-Parera2017} or HRIRs \citep{Hwang2008} \citep{Hwang2007} \citep{Fink2012}. There are benefits of working solely with HRIRs, one retains the interaural time difference (ITD), and it's easier to extract the effects of subject anthropometry on the resultant HRIR. However when analysing HRIRs with PCA, researchers often time-align the HRIRs before processing \citep{hwang2010customization}, losing information on ITD. Estimation of ITD is a comparatively trivial task, and when not relying on an anthropometric model for individualisation the correlation between HRIR and anthropometric features becomes irrelevant. One advantage of the use of HRTFs that cannot be overstated is the larger corpus of research already done on the work \citep{Holzl2014a}. 

In an early paper on the subject, Middlebrooks \citep{middlebrooks1992observations} found that no matter the database used, the amount of variance described by each PC, and the number of PCs required for a substantial reconstruction of the original HRTF was more or less equal. This is helpful, allowing the results of any investigation into a particular method to be applied semi-interchangably to new research. The number of PCs used for reconstruction varies a lot, however - anywhere from 4 \citep{martens1987principal} to 90 \citep{martens1987principal}. The decision as to how many PCs to use depends heavily on the intended accuracy of the reconstruction. The more PCs that are used, the more accurate any reconstruction based on them would be. There are, however, heavily diminishing returns on this number. Many studies are in agreement that using 4-6 PCs can describe around 90\% of the variance within the HRTF set \citep{martens1987principal} \citep{Kistler1992}. It is common to want to reduce the number of PCs used, so as to in turn reduce the complexity of the tuning process \citep{Hwang2007}. When this tuning process is manual, this concern/focus is understandable. But if the process is automated, there could perhaps be a greater focus on accurate reconstruction.

Holzl \citep{Holzl2012} investigated the effect that different input matrices, created by restructuring the HRTF input data, had when performing PCA on HRTFs. In doing this he identified five different matrix arrangements used by different studies, and found the most effective to be [(\textit{subjects * sound directions}) x \textit{signal}] - a structure used by Kistler and Wightman in their studies \citep{Kistler1992}. 

There is also a difference in how much of the dataset is analysed and adjusted at a time. Some studies \citep{•} [cite a bunch] elect to just adjust a clustered sub-set of positions, playing a subject a sample from that direction and adjusting just those positions based on that. This process of performing PCA and updating the weights for each position can be time consuming. Other studies \citep{Holzl2014a} use a more global model, analysing all directions at once. For this method to be practical when performing manual adjustments, a method for modeling PCWs can be helpful.

Holzl \citep{Holzl2014a} proposes a method for modelling PCWs based on the spherical harmonics transform. The proposed model effectively maps the PCWs to a sphere around the subject, allowing manipulation of the PCWs that apply to the intended apparent source of the sound being played to the user at a given time. There is a possibility that this approach could allow automated individualisation to be performed more effectively, adjusting the relevant datapoints in a more targeted way. 

Alternatively, PCWs can be manipulated directly \citep{•}. As a part of this project, some decisions will need to be made as to the efficacy of each approach when applied to my intended implementation method. 


a little on ways to match PCs with anthropometry 
anything else about the decomposition? 
then how to reconstruct HRTFs and the effect that the modification of PCWs has on the reconstructions and I think I should be good

\subsection{Search Methods}
My proposed method for producing HRTFs is based upon being able to automate the adjustment of the principal components/principal component weights within produced by using PCA on a generic set, all within a VR/AR environment, to reduce localisation errors for the user. This can, then, be framed as an optimisation problem. If a user is played multiple samples that have been filtered through HRTFs in sequence and, upon being asked, manages to identify the source of the sound within a sufficiently small margin of error then the HRTF set can be considered to fit the individual. The problem with developing a method for individualisation that works in this way is that there is little in the way of research that investigates which PCWs help to lower error rates in which directions. Because of this, I have instead chosen to investigate existing search algorithms to apply to this problem. 

\subsubsection{Simulated Annealing}
Simulated Annealing \citep{vanLaarhoven1987} search is a simple iterative search method that requires a heuristic that measures how close a given state is to the goal state. The steps the algorithm takes are essentially as follows:
\begin{itemize}
\item From a starting state - in our case a generalised HRTF.
\item The state is then evaluated by some evaluation function - in this case how close the user got to localising the set of samples.
\item A new pseudorandom state is generated - in this case each PCW would be modified by a random amount, with the degree of randomness depending on the following:
\begin{itemize}
\item If the state was close to the goal state (if the user almost correctly identified the source of the samples) then use less randomness when generating successive states - use a smaller boundary when generating random amounts to adjust by.
\item Otherwise, generate a new state and start from there - allow greater variance in the values that are used to adjust the weights. 
\end{itemize}
\item This successive state - a modified HRTF set - should again be tested according to the evaluation function, as the process loops. 
\end{itemize}

This process allows for an HRTF set to be individualised using PCA without prior knowledge about the relationship between error rates and principal components. Some adjustments may need to be made, however, in order to increase the efficiency of the algorithm. If information about states and the error rates they produce is saved, it may help to avoid the problem of pursuing modifications that don't actually get closer to the goal state. Having the ability to roll-back changes could be beneficial. This record of how much PCWs are adjusted and the error rates that are produced by those adjustments may help to add a bias to subsequent adjustments made to PCWs. This could limit the amount of randomness required in the adjustments, and may help the algorithm to achieve its goal state faster. 

This method is of course sub-optimal, because of its loose correlation between the error rates and the adjustments made to the PCs/PCWs. Until those relationships can be studied further, however, it may be an acceptable compromise in order to produce a proof of concept. There is a possibility that the data generated by this project could be used to investigate such a correlation, and as such a later update to the tool may be able to increase its efficacy. 